from abc import ABC, abstractmethod
from typing import List, Tuple, Union, Optional
import torch
import numpy as np
from pathlib import Path

from modular_env import ModularEnv
from stable_baselines3.common.vec_env.base_vec_env import *


class Engine(ABC):
    def __init__(self, asset_path:str, step_size: float, headless:bool=True) -> None:
        super().__init__()
        self.asset_path = asset_path  # Path to assets used in simulation
        self.headless = headless  # True if the simulation will not be rendered, otherwise false 
        self.step_size = step_size  # Amount of time passing each time .step() is called

    @abstractmethod
    def set_up(
        self, 
        env: ModularEnv
    ) -> Tuple[List[int], List[int], List [int]]:
        """
        The robot, obstacle and sensor class contains all pramameters about the objects which need to be spawned:
            - Position
            - Rotation
            - Mass
            - Collisions
        Depending on class:
            - (Robot:) Urdf containing load data, mass
            - (Obstacle:)
                - Mass, Colour
                - (Shape:) Cube/Sphere/Cylinder/Mesh and additional parameters

        Sensors are always free floating and not attatched to a robot/obstacle. If a sensor needs to be attatched,
        it is specified in the Robot/Obstacle class.

        Will create a default gound plane

        num_envs: Number of environments which will be simulated in paralles
        boundaries: Maximum amount of space required per environment
        set_size: Amount of time simulated per sim step

        Returns the generated robot, obstacles and sensor ids. All of them are integers, and no robot id equals any obstacle id
        """
        pass

    @abstractmethod
    def set_joint_positions(
        self,
        positions: Optional[Union[np.ndarray, torch.Tensor]],
        robot_indices: Optional[Union[np.ndarray, List, torch.Tensor]] = None,
        joint_indices: Optional[Union[np.ndarray, List, torch.Tensor]] = None,
    ) -> None:
        """
        Sets the joint positions of all robots specified in robot_indices to their respective values specified in positions.
        """
        pass
    
    @abstractmethod
    def set_joint_position_targets(
        self,
        positions: Optional[Union[np.ndarray, torch.Tensor]],
        robot_indices: Optional[Union[np.ndarray, List, torch.Tensor]] = None,
        joint_indices: Optional[Union[np.ndarray, List, torch.Tensor]] = None,
    ) -> None:
        """
        Sets the joint position targets of all robots specified in robot_indices to their respective values specified in positions.
        """
        pass

    @abstractmethod
    def set_joint_velocities(
        self,
        velocities: Optional[Union[np.ndarray, torch.Tensor]],
        robot_indices: Optional[Union[np.ndarray, List, torch.Tensor]] = None,
        joint_indices: Optional[Union[np.ndarray, List, torch.Tensor]] = None,
    ) -> None:
        """
        Sets the joint velocities of all robots specified in robot_indices to their respective values specified in velocities.
        """
        pass
     
    @abstractmethod   
    def set_joint_velocity_targets(
        self,
        velocities: Optional[Union[np.ndarray, torch.Tensor]],
        robot_indices: Optional[Union[np.ndarray, List, torch.Tensor]] = None,
        joint_indices: Optional[Union[np.ndarray, List, torch.Tensor]] = None,
    ) -> None:
        """
        Sets the joint velocities targets of all robots specified in robot_indices to their respective values specified in velocities.
        """
        pass

    @abstractmethod
    def set_local_poses(
        self,
        translations: Optional[Union[np.ndarray, torch.Tensor]] = None,
        orientations: Optional[Union[np.ndarray, torch.Tensor]] = None,
        indices: Optional[Union[np.ndarray, list, torch.Tensor]] = None,
    ) -> None:
        """
        Sets the local pose, meaning translation and orientation, of all objects (robots and obstacles)
        """
        pass

    @abstractmethod
    def get_local_poses(
        self,
        indices: Optional[Union[np.ndarray, list, torch.Tensor]] = None,
    ) -> Union[Tuple[np.ndarray, np.ndarray], Tuple[torch.Tensor, torch.Tensor]]:
        """
        Gets the local pose, meaning translation and orientation, of all objects (robots and obstacles)
        """
        pass

    @abstractmethod
    def get_sensor_data(
        self, 
        indices: Optional[Union[np.ndarray, list, torch.Tensor]] = None,
    ) -> List[List]:
        """
        Gets the sensor data generated by all sensors.
        """
        pass

    @abstractmethod
    def get_collisions(self) -> List[Tuple[str, str]]:
        """
        Returns the ids of objects which are colliding. Updated after each step.
        Example: [(Robot1, Robot2), (Robot1, Obstacle3)] -> Object 1 is colliding with object 2 and 3.
        """
        pass

    @abstractmethod
    def step(self):
        """
        Steps the environment for one timestep
        """
        pass

    @abstractmethod
    def get_observations(self) -> VecEnvObs:
        """
        Returns the obersavtions of all environments
        """
        pass

    @abstractmethod
    def get_robot_dof_limits(self) -> Union[np.ndarray, torch.Tensor]:
        """
        Returns:
            Union[np.ndarray, torch.Tensor]: degrees of freedom position limits. 
            shape is (N, num_dof, 2) where index 0 corresponds to the lower limit and index 1 corresponds to the upper limit.
            Only returns dof limits from the first environments, assuming all other environments contain duplicate robot configurations.

        """
        pass

    def _get_absolute_asset_path(self, urdf_path: str):
        return Path(self.asset_path).joinpath(urdf_path)